## Getting Started

Copy all the text on this screen. In R Studio's "File" menu, select "New File" then "R Markdown". Add a title like "SOAN 245 Workshop Week 6", include your name in the "Author" box, and click Ok. A default markdown file will open. Delete all the text in the default file below line 7 and paste in the text you copied to your clipboard. Make sure that "R Markdown" is selected in the file type drop down menu in the bottom right corner of this pane.

## Once Your File Is Saved


### 1. Introduction and Set Up

Today we will explore some of the approaches available to big data researchers interested in analyzing social media messages. We will use a database of tweets from President Donald Trump collected at the [Trump Twitter Archive](http://trumptwitterarchive.com). The tweets we will look at are all tweets sent by `@realdonaldtrump` in the year between October 17, 2017 and October 18, 2018.

To start, let's load the R packages we will need. Recall that in an R Markdown Notebook, you can click the green arrow in the chunk below to execute the commands.

```{r load packages, error=FALSE, message=FALSE, warning=FALSE}
install.packages(c("plyr", "tidyverse"))
library(plyr)
library(tidyverse)
```

We also need to load the dataset and clean up the `date` variable to make it more user-friendly:

```{r load data, message = FALSE, warning = FALSE, error = FALSE}
trump_tweets <- read.csv("https://raw.githubusercontent.com/lawrence-midd/soan245/2018/workshop_06_tweets.csv", 
                         stringsAsFactors = FALSE, 
                         fileEncoding="latin1")

trump_tweets$date <- as.Date(trump_tweets$date, "%m/%d/%y")
```


### 2. Descriptive Data

On what date did President Trump send the most tweets? First let's create a new data frame called `trump_tweets_date` with each date as an observation and a variable for the count of tweets by day:
```{r finding counts by date}
trump_tweets_date <- trump_tweets %>% group_by(date) %>% count()
```

Next, let's get a summary of the distribution of counts by looking at the range:
```{r summary of counts by date}
summary(trump_tweets_date$n)
```

So there was at least one day with 24 tweets! Is that many tweets unusual for him? To find out, let's plot the number of tweets by day over the last year:
```{r plot of counts by date}
trump_tweets_date <- as.data.frame(trump_tweets_date)
plot1 <- ggplot(trump_tweets_date, aes(x = date, y = n)) + geom_col() + 
     labs(title = "Number of Tweets by President Donald Trump by Day",
          subtitle = "October 17, 2017 to October 18, 2018", 
          x = "Date", y = "Number of Tweets") + ylim(c(0, 30))
plot1
```

That one day in late summer was pretty extraordinary. Which day was it? Here we want to pull the value for the `date` variable for the day when the count of tweets is the maximum value of that variable:

```{r date with most tweets}
trump_tweets_date$date[trump_tweets_date$n==max(trump_tweets_date$n)]
```

Let's look at all the tweets from that day. We'll start by pulling them into a new data frame called `most_tweets`.

```{r filter date with most tweets}
most_tweets <- filter(trump_tweets, date=="2018-08-29")
```

Now click on the spreadsheet icon for the `most_tweets` data frame in the top right pane to explore the tweets.


### 3. Tweets with Most and Least Retweets and Favorites

Let's go back to the data frame of all the tweets and find the date and text of the tweet with the most retweets. We'll use the same basic code to pull the values for the `date` and `text` variables for the observation with the max value for the `retweet_count` variable:

```{r most replies}
trump_tweets$date[trump_tweets$retweet_count==max(trump_tweets$retweet_count)]
trump_tweets$text[trump_tweets$retweet_count==max(trump_tweets$retweet_count)]
```

Try to find the date and text of the tweet with the lowest number of favorited tweets:

### REPLACE THIS LINE WITH YOUR CODE


### 4. Filtering By Content

Now let's find tweets that reference specific content. We'll start by finding the tweets that refer to "Fake News" and put them in a new data frame called `fakenews`. This new data frame will be a filter of the main dataset that only includes observations where the phrase "Fake News" shows up in the `text` field:

```{r filter tweets}
fakenews <- filter(trump_tweets, grepl("Fake News", text, useBytes = TRUE, ignore.case = TRUE))
```

Which tweet about Fake News got the most retweets? 


What other phrases or keywords could be interesting to search?



### 5. Sentiment Analysis

Beyond basic descriptives, there are tools to get a sense of the content of tweets. We'll try a common method of counting up the positive and negative words in a message and using them to calculate a "sentiment score."

To start, we need to load a file of positive words and a file of negative words:

```{r load positive and negative words, message = FALSE, warning = FALSE, error = FALSE}

pos.words <- scan("https://raw.githubusercontent.com/lawrence-midd/soan245/2018/workshop_06_positive_words.csv", what = "character")

neg.words <- scan("https://raw.githubusercontent.com/lawrence-midd/soan245/2018/workshop_06_negative_words.csv", what = "character")
```

What are some of the positive and negative words in these lists?

This next chunk sets up the function that will search all the tweets for individual words, count the positive and negative words, and calculate the scores. It's a lot of code...we won't go over it today but will need to run it.

```{r set up search function}

tweet <- as.vector(trump_tweets$text)
score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
 require(plyr)
 require(stringr)
 
scores = laply(sentences, function(tweet, pos.words, neg.words) {
 
 # clean up sentences with Râ€™s regex-driven global substitute, gsub():
 tweet = gsub('[[:punct:]]', '', tweet)
 tweet = gsub('[[:cntrl:]]', '', tweet)
 tweet = gsub('\\d+', '', tweet)
 # and convert to lower case:
 tweet = tolower(tweet)
 
 # split into words. str_split is in the stringr package
 word.list = str_split(tweet, '\\s+')
 # sometimes a list() is one level of hierarchy too much
 words = unlist(word.list)
 
 # compare our words to the dictionaries of positive & negative terms
 pos.matches = match(words, pos.words)
 neg.matches = match(words, neg.words)
 
 # match() returns the position of the matched term or NA
 # we just want a TRUE/FALSE:
 pos.matches = !is.na(pos.matches)
 neg.matches = !is.na(neg.matches)
 
 # and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
 score = sum(pos.matches) - sum(neg.matches)
 
 return(score)
 }, pos.words, neg.words, .progress=.progress )
 
 scores.df = data.frame(score=scores, text=tweet)
 return(scores.df)
}
```


Now let's create a data frame with the tweets and apply the function to that data frame. We'll also create a new data frame called `result` which will show the text of the tweet and the sentiment socre:
```{r run the function and save the scores}
##Put tweets in data frame
tweets.df <- as.data.frame(tweet)

##Apply sentiment function to the tweets
result <- score.sentiment(tweets.df$tweet,pos.words,neg.words)
```

It's time to see the distribution of sentiment scores...
```{r sentiment score summary}
summary(result$score)
```

How would you interpret this distribution?

We can also plot the distribution of sentiment scores...
```{r histogram of scores}
sentiment_plot1 <- ggplot(result, aes(x = score))
sentiment_plot1 + geom_bar(fill = "Blue") + 
     labs(title ="Distribution of Sentiment Scores in President Donald Trump's Tweets", 
          subtitle = "October 17, 2017 to October 18, 2018", 
          y = "Count of Tweets", x = "Sentiment Score")
```


### 6. Sentiment Analysis With Filtered Tweets

Let's put all our tools together by examining the sentiment scores for subsets of tweets filtered by key words and phrases. In this first example, we will look at tweets that inlcude the phrase "Fake News":

```{r tweets filtered by text}

trump_tweets_filter <- filter(trump_tweets, grepl("Fake News", text, useBytes = TRUE, ignore.case = TRUE))

tweet <- as.vector(trump_tweets_filter$text)

##Put tweets in data frame
tweets.df <- as.data.frame(tweet)

##Apply sentiment function to the tweets
result <- score.sentiment(tweets.df$tweet,pos.words,neg.words)

##Summarize scores
summary(result$score)

##Histogram of scores
sentiment_plot1 <- ggplot(result, aes(x = score))
sentiment_plot1 + geom_bar(fill = "Blue") + 
     labs(title ="Distribution of Sentiment Scores in President Donald Trump's Tweets Referencing `Fake News`", 
          subtitle = "October 17, 2017 to October 18, 2018", 
          y = "Count of tweets", x = "Sentiment Score")
```


Finally, let's look at the distribution of sentiment scores for the 100 most retweeted tweets:

```{r tweets filtered by other variables}

##This only keeps the 100 tweets with the most retweets
trump_tweets_filter2 <- trump_tweets %>% arrange(desc(retweet_count)) %>% top_n(100, retweet_count)
tweet <- as.vector(trump_tweets_filter$text)

##Put tweets in data frame
tweets.df <- as.data.frame(tweet)

##Apply sentiment function to the tweets
result <- score.sentiment(tweets.df$tweet,pos.words,neg.words)

##Summarize scores
summary(result$score)

##Histogram of scores
sentiment_plot1 <- ggplot(result, aes(x = score))
sentiment_plot1 + geom_bar(fill = "Blue") + 
     labs(title ="Distribution of Sentiment Scores in President Donald Trump's 100 Most Retweeted Tweets", 
          subtitle = "October 17, 2017 to October 18, 2018", 
          y = "Count of tweets", x = "Sentiment Score")
```
